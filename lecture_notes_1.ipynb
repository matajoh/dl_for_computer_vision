{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import itertools\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from ipywidgets import interact, interactive, fixed, IntSlider\n",
    "from IPython.display import display, Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Convolutional Neural Nets\n",
    "### University of Cambridge\n",
    "### Engineering Part IIB/Module 4F12: Computer Vision\n",
    "### Deep Learning for Computer Vision\n",
    "\n",
    "*Lecturer: Matthew Johnson*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we are going to focus on a kind of neural net which has seen some amazing successes in recent years. As you remember from the Introduction lecture, this system is capable of recognizing thousands of object categories, recognizing faces, detection people and their emotions, and even writing human-level captions to describe arbitrary images. This is all enabled through the use of a combination of a series of engineering techniques that enable deep learning along with a new type of layer: the _convolutional layer_.\n",
    "\n",
    "Convolution is a concept that comes from signal processing. One takes a audio or visual signal, usually from a noisy sensor, and then applies a _kernel_ at multiple points in that signal to enable easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Video(\"https://lectures.blob.core.windows.net/media/conv.mp4\", width=980, height=490))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "For the purpose of this lecture, we are going to consider convolution with arbitrary filters. We will start by looking at some standard filters used in image processing:\n",
    "\n",
    "1. Blur\n",
    "2. Edge\n",
    "3. Bar\n",
    "4. Blob\n",
    "\n",
    "For each of these we define a kernel that is convolved with the image. We can see the kernels below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import filters\n",
    "\n",
    "filters.show_kernels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blur\n",
    "As you can see, the \"hot spots\" of the response correspond to the center of the strokes made by the writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters.show_convolution(filters.blur_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal Edges\n",
    "Note how the blue responses indicate a black-to-white edge, and the red responses indicate a white-to-black edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters.show_convolution(filters.horizontal_edge_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal Bars\n",
    "See here how the filter fires on the top of the 5, and the cross bar of the 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters.show_convolution(filters.horizontal_bar_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blobs\n",
    "First, note how the blue response from the blob kernel follows the contour of the number very clearly. Then note the spots of dark red that show up in black areas that are at the center of a ring of white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters.show_convolution(filters.blob_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the orientations and scales of these basic features, an image can be processed to find all kinds of low-level structures that have proven useful for a variety of tasks in computer vision. A combination of filters like this is called a \"filter bank\". The image below depicts a famous selection of filters known as the Leung-Malik filter bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter banks and biological vision\n",
    "\n",
    "![Leung-Malik Filter Banks](https://lectures.blob.core.windows.net/media/lmfilters.jpg \"Leung Malik Filter Bank\")\n",
    "\n",
    "We see some familiar shapes here: edge filters in the top left, bar filters in the top right, blob filters in the bottom left and low-pass filters in the bottom right. What is so interesting about these filters is that they can also be detected in the vision systems of mammallian brains. Filters of this kind seem to be part of a working, intelligent vision system. Given that, how can we incorporate these filters into our artificial neural nets for use in image processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers\n",
    "\n",
    "Below is a very simple image $i$, along with two simple convolutional filters $w$ and their corresponding outputs $o$:\n",
    "\n",
    "![Convolution Example](https://lectures.blob.core.windows.net/media/convolutionExample.png \"Convolution Example\")\n",
    "\n",
    "Each output node is computed in the following way:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "o_{f,r,c} &= w_{f00}i_{r-1,c-1} + w_{f01}i_{r-1,c} + w_{f02}i_{r-1,c+1} \\\\\n",
    "         &~~+ w_{f10}i_{r,c-1} + w_{f11}i_{r,c} + w_{f12}i_{r,c+1} \\\\\n",
    "         &~~+ w_{f20}i_{r+1,c-1} + w_{f21}i_{r+1,c} + w_{f22}i_{r+1,c+1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples of convolution shown above, the values of $w$ were fixed to achieve particular effects that human engineers thought would be useful. What we want to do is to enable a neural net to learn these values automatically from the data use back-propagation, just like with the fully-connected layers we saw in the Multi-Layer Perceptron lecture. What makes this tricky, though, is that unlike the previous layers we have seen, the weights will touch multiple input values as they are convolved over the image. This means there are fewer parameters to train overall, but it complicates training considerably. We can solve this problem, however, by using more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## im2col\n",
    "\n",
    "Let's start with an image $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\left(\n",
    "         \\begin{array}{ccccc}\n",
    "           a & b & c & d & e \\\\\n",
    "           f & g & h & i & j \\\\\n",
    "           k & l & m & n & o \\\\\n",
    "           p & q & r & s & t \\\\\n",
    "           u & v & w & x & y \\\\\n",
    "         \\end{array}\n",
    "     \\right)\n",
    "$$\n",
    "\n",
    "In this figure, each pixel of the image corresponds to a letter. We are going to extract all of the patches and embed them as the columns of a larger matrix $\\mathbf{X}$ like so:\n",
    "\n",
    "![Im2Col](https://lectures.blob.core.windows.net/media/im2col.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now store the weights in a normal weight matrix, in which each row now operates as a convolution filter and perform convolution over the entire image as a simple matrix multiplication:\n",
    "\n",
    "$$\n",
    "\\mathbf{F} = \\left[\n",
    "         \\begin{array}{cccccccc}\n",
    "            w_{000} & w_{001} & w_{002} & w_{010} & w_{011} & \\ldots & w_{021} & w_{022} \\\\\n",
    "            w_{100} & w_{101} & w_{102} & w_{110} & w_{111} & \\ldots & w_{121} & w_{122} \\\\\n",
    "         \\end{array}\n",
    "       \\right]\n",
    "$$\n",
    "\n",
    "![Convolution as Multiplication](https://lectures.blob.core.windows.net/media/conv_mult.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(x, size):   \n",
    "    rows, cols = x.shape\n",
    "    num_patches = (rows - size + 1)*(cols - size + 1)\n",
    "    patch_size = size * size\n",
    "    X = np.zeros((patch_size, num_patches), dtype='float32')\n",
    "    patch = 0\n",
    "    for r in range(rows - size + 1):\n",
    "        for c in range(cols - size + 1):\n",
    "            X[:,patch] = x[r:r+size,c:c+size].reshape(patch_size)\n",
    "            patch += 1\n",
    "            \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(25).reshape(5,5)\n",
    "X = im2col(x, 3)\n",
    "\n",
    "print(\"x =\\n\",x)\n",
    "print(\"im2col(x, 3) =\\n\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try looking at replacing the first layer of a Multi-Layer Perceptron with a convolutional layer in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 9, 5)\n",
    "        self.f3 = nn.Linear(5184, 10)\n",
    "\n",
    "        self.outputs = [0] * 4\n",
    "        self.num_layers = 4\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        h = self.c1(x)\n",
    "        h = h.tanh().flatten(1)\n",
    "        output = self.f3(h)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, PyTorch provides us with an easy-to-use Conv2d layer. Let's train the model and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cnn\n",
    "from datasets import MulticlassDataset\n",
    "\n",
    "simple_net_mnist = cnn.SimpleCNN()\n",
    "dataset_mnist = MulticlassDataset.mnist(28).to_torch()\n",
    "path = \"mnist_simple_cnn.results\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    results = torch.load(path)\n",
    "    print(path, \"loaded\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    snapshots = cnn.train_model(dataset_mnist, simple_net_mnist, criterion, num_epochs=5)\n",
    "    results = {\"snapshots\": snapshots, \"net\": simple_net_mnist.state_dict()}\n",
    "    torch.save(results, path)\n",
    "\n",
    "snapshots = results[\"snapshots\"]\n",
    "simple_net_mnist.load_state_dict(results[\"net\"])\n",
    "cnn.evaluate_model(dataset_mnist, simple_net_mnist, snapshots)\n",
    "cnn.visualize_layer_output(simple_net_mnist, 1, 3, [3, 4, 3], 4)\n",
    "cnn.visualize_layer_output(simple_net_mnist, 3, 3, [3, 4, 3], 4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned filters are recognisable to us as oriented bars, edges, and corners, but have been tailored to the dataset and object. A different objective function or dataset may result in different features. However, we have created a new problem. Let's look at this network as a diagram:\n",
    "\n",
    "![MNIST CNN](https://lectures.blob.core.windows.net/media/cnn_nopool.svg)\n",
    "\n",
    "In the original MLP the final layer had $64 \\times 10 = 640$ weights. Since each convolution produces an image, the final layer here has $9 \\times 24 \\times 24 \\times 10 = 51,840$ weights. We need a way to reduce the dimensionality of the filter layer output.\n",
    "\n",
    "For this, we shall first return to the simple filters we discussed at the beginning of the lecture. A low-pass filter has some interesting properties apart of reducing noise. Because it mixes together samples from a spatial area, it allows us to subsample the image in space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters.show_convolution(filters.blur_kernel, stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the resulting image is scaled down to half the size, or a quarter of the pixels, of the original. How can we incorporate this into CNN training? We are going to introduce a new layer called a _pooling layer_. The concept of pooling is similar to sub-sampling, whereby an image is reduced in size by representing a region in the input image via a single pixel. In standard sub-sampling this value is a single pixel in the input region, _e.g._ the center. The first kind of pooling we are going to look at is average pooling, where the value is the mean value of the pixels in the input region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen below a filter of size 3 is applied at a _stride_ of 2, though the size and stride can be altered in practice as needed by the problem. This pooling acts as a _non-parameterised_ layer in the neural network, much like non-linearities. An output value is computed as follows:\n",
    "\n",
    "$$o[r,c] = \\frac{1}{9}\\sum_{k = 0}^{3} \\sum_{l = 0}^{3} i[2r + k - 1, 2c + l - 1]$$\n",
    "\n",
    "This is equivalent to using the embedding function _im2col_ and then multiplying by a weight matrix:\n",
    "\n",
    "$$\\mathbf{W} = \\left\\{W_{i,j} | \\forall i \\forall j ~ W_{i,j} = \\frac{1}{9}\\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pooling\n",
    "\n",
    "pooling.avg_pool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to average pooling is maximum, or max, pooling. As with average pooling, the max pooling operator\n",
    "has a set size and is applied at a predetermined stride. However, instead of taking the arithmetic mean of all of the\n",
    "pixel values it selects only the maximum value as its output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling.max_pool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens to our CNN when we add in max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoolingCNN, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 9, 5)\n",
    "        self.f3 = nn.Linear(1296, 10)\n",
    "\n",
    "        self.num_layers = 4\n",
    "        self.outputs = [0] * self.num_layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self.c1(x)\n",
    "        h = F.max_pool2d(h, 2, stride=2)\n",
    "        h = h.tanh()\n",
    "        output = self.f3(h.flatten(1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_pooling_cnn = cnn.PoolingCNN()\n",
    "path = \"mnist_pooling_cnn.results\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    results = torch.load(path)\n",
    "    print(path, \"loaded\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    snapshots = cnn.train_model(dataset_mnist, mnist_pooling_cnn, criterion, num_epochs = 5)\n",
    "    results = {\"snapshots\": snapshots, \"net\": mnist_pooling_cnn.state_dict()}\n",
    "    torch.save(results, path)\n",
    "\n",
    "mnist_pooling_cnn.load_state_dict(results[\"net\"])\n",
    "snapshots = results[\"snapshots\"]\n",
    "cnn.evaluate_model(dataset_mnist, mnist_pooling_cnn, snapshots)\n",
    "cnn.visualize_layer_output(mnist_pooling_cnn, 1, 3, [3, 4, 3], 4)\n",
    "cnn.visualize_layer_output(mnist_pooling_cnn, 3, 3, [3, 4, 3], 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram of this network:\n",
    "\n",
    "![MNIST Pooling CNN](https://lectures.blob.core.windows.net/media/cnn.svg)\n",
    "\n",
    "This model has quarter the number of parameters as a result of the max pooling for the output layer ($9\\times12\\times12\\times10 = 12,960$). As you chain together convolutional layers and max pooling layers and the dimensionality of the images decreases, more and more high-level features of the image can be detected and used for classification by the final layer.\n",
    "\n",
    "We are quickly exhausting the possibilities of the MNIST dataset. For the remainder of this lecture we will be working with the CIFAR-10 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "CIFAR-10, while it has many of the same characteristics as MNIST, is a considerable step up in difficulty. It is drawn from the \"80 million tiny images\" dataset which consists of (almost) 80 million $32 \\times 32$ RGB images downloaded from the internet and labeled with one of 75,062 non-abstract nouns in English.\n",
    "\n",
    "![CIFAR-10](https://lectures.blob.core.windows.net/media/cifar10.jpg \"CIFAR-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 (named after the Canadian Institute for Advanced Research where it was gathered) consists of\n",
    "60,000 images gathered from this larger set by gathered by Alex Krizhevsky, Vinod Nair and\n",
    "Geoffrey Hinton.  These images belong to 10 classes: (in order from left to right above)\n",
    "airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck. As can be seen, the dataset contains\n",
    "a lot of intra-class variance, in addition to complications brought about by color and the much larger variety inherent\n",
    "in non-curated natural images.\n",
    "\n",
    "What kind of network do we need in order to deal with this much more difficult dataset? Using the building blocks we have seen in this lecture, we can now fully examine the anatomy of a modern Deep Neural Net or DNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CIFAR-10 DNN](https://lectures.blob.core.windows.net/media/dnn.svg \"CIFAR-10 DNN\")\n",
    "\n",
    "<table>\n",
    "  <tr><th>C1</th><td>Extracts low-level features in the image.</td></tr>\n",
    "  \n",
    "  <tr><th>P2</th><td>Provides some flexibility of location</td></tr>\n",
    "  \n",
    "  <tr><th>C3</th><td>Looks for parts that are combinations of features</td></tr>\n",
    "  \n",
    "  <tr><th>P4</th><td>Smooths the part responses before subsampling</td></tr>\n",
    "  \n",
    "  <tr><th>C5</th><td>Finds structures that are built from parts</td></tr>\n",
    "  \n",
    "  <tr><th>P6</th><td>SSmooths and subsamples the structural responses.</td></tr>\n",
    "      \n",
    "  <tr><th>F7</th><td>Sub-category latent space</td></tr>\n",
    "  \n",
    "  <tr><th>F8</th><td>Final classifier</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CifarCNN, self).__init__()\n",
    "        self.c1=nn.Conv2d(3, 32, 5, stride=1, padding=2)\n",
    "        self.c3=nn.Conv2d(32, 32, 5, stride=1, padding=2)\n",
    "        self.c5=nn.Conv2d(32, 64, 5, stride=1, padding=2)            \n",
    "        self.f7=nn.Linear(1024, 64)\n",
    "        self.f8=nn.Linear(64, 10)\n",
    "\n",
    "        self.num_layers = 9\n",
    "        self.outputs = [0] * self.num_layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.outputs[0] = x\n",
    "        self.outputs[1] = h = self.c1(x)\n",
    "        self.outputs[2] = h = F.max_pool2d(h, 3, stride=2).relu()\n",
    "        self.outputs[3] = h = self.c3(h).relu()\n",
    "        self.outputs[4] = h = F.avg_pool2d(h, 3, stride=2, padding=1)\n",
    "        self.outputs[5] = h = self.c5(h).relu()\n",
    "        self.outputs[6] = h = F.avg_pool2d(h, 3, stride=2, padding=1)\n",
    "        self.outputs[7] = h = self.f7(h.flatten(1))\n",
    "        self.outputs[8] = h = self.f8(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cifar = MulticlassDataset.cifar().normalize()\n",
    "cifar10_cnn = cnn.CifarCNN()\n",
    "path = \"cifar10.results\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "image = dataset_cifar.unnormalize(dataset_cifar.val.values[0])\n",
    "dataset_cifar = dataset_cifar.to_torch()\n",
    "\n",
    "if os.path.exists(path):\n",
    "    results = torch.load(path)\n",
    "    print(path, \"loaded\")\n",
    "else:\n",
    "    print(\"Running on\", device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    snapshots = cnn.train_model(dataset_cifar, cifar10_cnn, criterion, device = device)\n",
    "    results = {\"snapshots\": snapshots, \"net\": cifar10_cnn.state_dict()}\n",
    "    torch.save(results, path)\n",
    "\n",
    "cifar10_cnn.load_state_dict(results[\"net\"])\n",
    "snapshots = results[\"snapshots\"]\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = np.swapaxes(image, 0, 1)\n",
    "image = np.swapaxes(image, 1, 2)\n",
    "plt.imshow(image, interpolation='nearest')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "\n",
    "cifar10_cnn.load_state_dict(results[\"net\"])\n",
    "snapshots = results[\"snapshots\"]\n",
    "cnn.evaluate_model(dataset_cifar, cifar10_cnn, snapshots, device=device)\n",
    "cnn.visualize_layer_output(cifar10_cnn, 1, 0, [3, 4, 3], 4)\n",
    "cnn.visualize_layer_output(cifar10_cnn, 3, 0, [3, 4, 3], 4)\n",
    "cnn.visualize_layer_output(cifar10_cnn, 5, 0, [3, 4, 3], 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the features learned by these more complex networks are harder to analyze and understand. While the features at the beginning map to the simple filters we used at the beginning of the lecture, the filters in later layers are hard to understand. That said, from the output images we can clearly see that they are pulling out parts of input object, identifying certain structures that are later used to perform classification.\n",
    "\n",
    "Now that we have examined a deep net operating on the CIFAR-10 dataset, we are going to end the lecture by looking at the state of the art in Deep Convolutional Neural Nets. To do that, however, we are going to have to introduce one final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep CNN Training\n",
    "\n",
    "1. Divide the dataset into training $\\mathbf{D}$, validation $\\mathbf{V}$ and test $\\mathbf{T}$ partitions\n",
    "2. Compute the \"mean\" image of $\\mathbf{D}$, and subtract it from all images\n",
    "3. Scale all images so that pixel values are in the range $[-1,1]$\n",
    "4. Design a network architecture, or adapt a known-good architecture to the dataset\n",
    "5. Select an optimisation algorithm (_i.e._ a version of SGD)\n",
    "6. For a pre-determined set of epochs, do the following:\n",
    "    1. Randomly sample (without replacement) $\\mathbf{D}_B$ images from $\\mathbf{D}$\n",
    "    2. Compute $\\nabla \\mathbf{W}^{\\tau}$ for all $\\mathbf{W}$ in the model\n",
    "    3. Update all $\\mathbf{W}^{\\tau} \\rightarrow \\mathbf{W}^{\\tau + 1} $\n",
    "    4. Once all images in $\\mathbf{D}$ have been seen (_i.e._ when an epoch is complete), evaluate the network on $\\mathbf{D}$ and $\\mathbf{V}$ to monitor changes in the loss\n",
    "7. Evaluate final performance on $\\mathbf{V}$, and repeat the above with different optimisation hyperparameters as necessary\n",
    "8. Retrain the network using $\\mathbf{D} \\cup \\mathbf{V}$ with the final hyperparameters, and evaluate on $\\mathbf{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Sometimes we want to augmentation a dataset. One way of doing that is through simple image transforms:\n",
    "\n",
    "![Data Augmentation](https://lectures.blob.core.windows.net/media/data_augmentation.png \"Data Augmentation\")\n",
    "\n",
    "In this way one training image can become 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to deep networks, the only thing better than a lot of data is a whole lot of data. However, gathering\n",
    "supervised data is very expensive. One way to overcome this problem is via _data augmentation_, whereby on each epoch each\n",
    "training image is turned into several additional images by way of translated cropping and horizontal reflection as seen above.\n",
    "    \n",
    "Another common usage of data augmentation is during the testing phase, whereby instead of simply computing $P(\\mathbf{x}=i)$ from a single image, $\\mathbf{x}$ is altered as shown above and then each version of $\\mathbf{x}$ is shown to the network. In this scenario, the predicted label is then:\n",
    "\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "i = \\argmax_j \\left\\{j_a | \\forall a: j_a = \\argmax_k P(\\mathbf{x}_a=k) \\right\\}\n",
    "$$\n",
    "\n",
    "This can result in significant improvement regardless of the network model used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalisation\n",
    "\n",
    "Batch Normalisation is a technique that is essential to the deepest neural net architectures, but also useful in general for stabilising and accelerating the training process in all circumstances. The concept is straightforward: if we knew the statistics of each layer's output over the entire dataset, we could normalise those outputs so that all output vectors had zero mean and unit variance. In mathematical terms, we want to do the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_{\\mathbf{D}} &= \\frac{1}{|\\mathbf{D}|}\\sum_{\\mathbf{x} \\in \\mathbf{D}} \\mathbf{x} \\\\\n",
    "\\sigma^2_{\\mathbf{D}} &= \\frac{1}{|\\mathbf{D}|}\\sum_{\\mathbf{x} \\in \\mathbf{D}} (\\mathbf{x} - \\mu_{\\mathbf{D}})^{2} \\\\\n",
    "\\hat{\\mathbf{x}} &= \\frac{\\mathbf{x} - \\mu_{\\mathbf{D}}}{\\sqrt{\\sigma^2_{\\mathbf{D}}} + \\epsilon}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The problem is that we do not know $\\mu_{\\mathbf{D}}$ or $\\sigma^2_{\\mathbf{D}}$, nor can we compute them as they will change as the network itself changes during training. We overcome this by using the same batch trick to estimate these values _i.e._ by computing them for each batch during training. Once the network is trained, we can then compute the true values and use those during testing. We can even learn training parameters for scaling and shifting the normalised vectors:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\gamma\\hat{\\mathbf{x}} + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNet\n",
    "\n",
    "The ImageNet dataset consists of over 14 million images mapped to around 20,000 labels. The images are extremely varied, coming from a variety of different sources all over the internet, and have each been hand-labeled by a human. \n",
    "\n",
    "![ImageNet](https://lectures.blob.core.windows.net/media/imagenet.jpg \"ImageNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ImageNet Samples](https://lectures.blob.core.windows.net/media/imagenet_samples.png \"ImageNet Samples\")\n",
    "\n",
    "Left | Center | Right\n",
    "--- | --- | ---\n",
    "Handheld Computer | Dandie Dinmont Terrier | Teapot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "\n",
    "When ImageNet was first introduced, it was prohibitively expensive to train a neural network on it. However, in 2012 Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton introduced AlexNet, a CNN architecture that was able to achieve a top-5 error rate of 15.3% on the ILSVRC 2012 dataset, a 10.8% improvement over the previous state-of-the-art. To achieve this they harnessed a new technology for use in training DNNs.\n",
    "\n",
    "![AlexNet Diagram](https://lectures.blob.core.windows.net/media/alexnet.svg)\n",
    "\n",
    "Graphical Processing Units, or GPUs, were originally designed for graphically intensive workflows, like video games. In particular, they are specialised towards the execution of per-pixel code in which linear algebra is used to project 3D model information to the camera plane. One can coerce the hardware into performing other computationally intensive image tasks, *e.g.* those in computer vision. As a demonstrator, see this implementation of Canny edge detection as using GPU shaders:\n",
    "\n",
    "[Canny Edge Detection with shaders](https://matajoh.github.io/canny/)\n",
    "\n",
    "Krizhevsky *et al.* realised that this same hardware could be used to perform the matrix multiplications required by CNNs, and that the parallel nature of GPUs would allow them to train much faster than on CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Networks\n",
    "\n",
    "Even with all the tricks we have introduced so far, very deep networks fail to train due to a version of the vanishing gradient problem. In 2015, Kaiming He *et al.* introduced Residual Networks, a new architecture that was able to achieve a top-5 error rate of 3.57% on the ILSVRC 2012 dataset despite being 152 layers deep. Here is the 34-layer configuration:\n",
    "\n",
    "![ResNet](https://lectures.blob.core.windows.net/media/resnet.svg)\n",
    "\n",
    "The key contribution is the repeating *residual block* within this diagram. It has two branches: an identity branch which copies the input, and the following miniature network:\n",
    "\n",
    "![ResNet Block](https://lectures.blob.core.windows.net/media/resblock.svg)\n",
    "\n",
    "The identity branch acts as a shortcut, allowing the network to ignore large blocks of its capacity if they are not needed. In essence, the network grows deeper each time a residual block saturates during training.\n",
    "\n",
    "One really helpful feature of PyTorch is the ability to easily download pre-trained models. Here's some code that runs a ResNet-50, trained on ImageNet, on images from the COCO dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "\n",
    "num_images = 32\n",
    "batch_size = 8\n",
    "dataset_coco = load_coco(\"minival\")\n",
    "images_coco = dataset_coco[\"images\"][:num_images]\n",
    "categories, images, top5 = resnet.classify(images_coco, batch_size)\n",
    "\n",
    "def plot(fig: plt.Figure, i: int):\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.imshow(images[i])\n",
    "    ax.axis('off')\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    width = [top5[i][j].score for j in range(5)][::-1]\n",
    "    names = [categories[top5[i][j].id].replace(\" \", \"\\n\") for j in range(5)][::-1]\n",
    "    ax.barh(range(5), width, tick_label=names)\n",
    "    ax.set_xlim(0, 1)\n",
    "    fig.tight_layout()\n",
    "\n",
    "def slide_resnet(frame: int):\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    plot(fig, frame)\n",
    "\n",
    "interact(slide_resnet, frame=IntSlider(value=0, min=0, max=num_images-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "livereveal": {
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
